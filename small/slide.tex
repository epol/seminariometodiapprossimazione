\documentclass{beamer}

\usetheme{metropolis}

\metroset{sectionpage=progressbar,progressbar=frametitle}
\metroset{numbering=fraction}
\metroset{block=fill}

\usepackage{pgfpages}
%\setbeameroption{show notes on second screen}

\usepackage{appendixnumberbeamer}

\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xfrac}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{mathtools}

% Plot e disegnini
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}

% to do small font tabular columns
\usepackage{array}


\newcounter{counter1}

\theoremstyle{plain}
\newtheorem{myteo}[counter1]{Theorem}
\newtheorem{mylem}[counter1]{Lemma}
\newtheorem{mypro}[counter1]{Proposition}
\newtheorem{mycor}[counter1]{Corollary}
%\newtheorem*{myteo*}{Teorema}
%\newtheorem*{mylem*}{Lemma}
%\newtheorem*{mypro*}{Proposizione}
%\newtheorem*{mycor*}{Corollario}

\theoremstyle{definition}
\newtheorem{mydef}[counter1]{Definition}
\newtheorem{myes}[counter1]{Example}
%\newtheorem{myex}[counter1]{Esercizio}
%\newtheorem*{mydef*}{Definizione}
%\newtheorem*{myes*}{Esempio}
%\newtheorem*{myex*}{Esercizio}

\theoremstyle{remark}
%\newtheorem{mynot}[counter1]{Nota}
\newtheorem{myoss}[counter1]{Remark}
%\newtheorem*{mynot*}{Nota}
%\newtheorem*{myoss*}{Osservazione}

\newcommand{\obar}[1]{\overline{#1}}
\newcommand{\ubar}[1]{\underline{#1}}

\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\ang}[1]{\left<#1\right>}
\newcommand{\bra}[1]{\left[#1\right]}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}

\newcommand{\pfrac}[2]{\pa{\frac{#1}{#2}}}
\newcommand{\bfrac}[2]{\bra{\frac{#1}{#2}}}
\newcommand{\psfrac}[2]{\pa{\sfrac{#1}{#2}}}
\newcommand{\bsfrac}[2]{\bra{\sfrac{#1}{#2}}}

\newcommand{\der}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pder}[2]{\pfrac{\partial #1}{\partial #2}}
\newcommand{\sder}[2]{\sfrac{\partial #1}{\partial #2}}
\newcommand{\psder}[2]{\psfrac{\partial #1}{\partial #2}}

\newcommand{\intl}{\int \limits}

\DeclareMathOperator{\de}{d}
\DeclareMathOperator{\id}{Id}
\DeclareMathOperator{\len}{len}

\DeclareMathOperator{\gl}{GL}
\DeclareMathOperator{\aff}{Aff}
\DeclareMathOperator{\isom}{Isom}

\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\sign}{sign}




\title{Altre iterazioni per la radice quadrata di matrici}
\date{\today}
\author{Enrico Polesel}
%\institute{Universit\`a di Pisa}


\begin{document}
\maketitle

\section{Definition}

\begin{frame}{Principal square root}
  \begin{columns}[T]
    \begin{column}{.45\textwidth}
      For $z\in \mathbb{C}$ the equation $y^2 = z$ has two solutions $y_1,
      y_2$, when $z\not\in \mathbb{R}^-$ we call \textit{principal
        square root} the $y$ such that $\re\pa{y} >0$.
      \vspace{15pt}
      
      If $A \in \mathbb{C} ^ {n \times n}$ is a non singular matrix
      with no eigenvalues on $\mathbb{R}^-$ then there is one
      principal square root of $A$ and we denote it as $A^{1/2}$.
      \vspace{8pt}

      It can be extended to singular matrices with semisimple zero
      eigenvalues.
    \end{column}
    \begin{column}{.6\textwidth}
      \begin{figure}
        \begin{tikzpicture}
          \begin{axis}[axis lines=middle,axis equal,xmin=-3.5,xmax=2.5]
            \addplot [blue,data cs=polar] coordinates{(0,0) (140,4)};
            \node at (axis cs:-3.2,2.4) {$z$};
            \addplot [red,data cs=polar] coordinates{(0,0) (70,2)};
            \addplot [red,data cs=polar] coordinates{(0,0) (250,2)};
            \node at (axis cs:1,2) {$y_1$};
            \node at (axis cs:-1,-2) {$y_2$};
          \end{axis}
        \end{tikzpicture}  
      \end{figure}
    \end{column}
  \end{columns}
\end{frame}

% \begin{frame}{Schur Method}
%   Let $A \in \mathbb{C}^{n\times n}$ be a nonsingolar matrix without
%   eigenvalues in $\mathbb{R}^-$, given a Schur decomposition
%   $A = QTQ^*$ we compute $A^{1/2} = Q U Q^*$ with $U$ such that:
%   \begin{align*}
%     u_{ii}^2 &= t_ii \\
%     \pa{u_{ii} + u_{jj}} u_{ij} &= t_{ij} - \sum _{k=i+1} ^{j-1}
%                                   u_{ik}u_{kj}
%   \end{align*}
  
%   \begin{block}{Cost}
%     $\pa{ 28+\frac{1}{3}} n^3$ floating point operations.
%   \end{block}
% \end{frame}


\section{Iterative methods}

\subsection{Newton iteration}

% \begin{frame}{``Full'' Newton's method}
%   \[ A = \pa{Y+E}^2 = Y^2 + YE + EY + E^2 \]
%   \begin{align*}
%     X_k E_k + E_k X_k &= A - X_k ^2 \\
%     X_{k+1} &= X_k + E_k
%   \end{align*}
  
%   Solving the Sylvester equation for $E_k$ at every step we obtain a
%   succession of matrices $\pa{ X_k} _{k\in \mathbb{N}}$.

%   \begin{myoss}
%     We need to resolve a Sylvester equation for each iteration, so we
%     pay at least $25n^3$ flops (a Schur decomposition) every time!
%   \end{myoss}
% \end{frame}

% \begin{frame}
%   If $X_k$ commutes with $E_k$ we can write
%   \[ E_k = \frac{1}{2} X_k ^{-1} \pa{ A - X_k ^2} \Rightarrow X_{k+1}
%     =  \frac{1}{2} \pa{ X_k ^{-1} A + X_k } \]
%   \pause
%   \begin{mylem}
%     Suppose that in the Newton iteration $X_0$ commutes with $A$ and
%     all the iterates are well-defined. Then, for all $k$, $X_k$
%     commutes with $A$ and
%     \[ X_{k+1} = \frac{1}{2} \pa{ X_k + X_k ^{-1} A} \]
%   \end{mylem}
%   \begin{proof}
%     By induction.
%   \end{proof}
% \end{frame}

\begin{frame}{Newton's method}
  \begin{block}{Newton iteration}
    \[ \left\{
        \begin{matrix}
          X_0 &=& A \\
          X_{k+1} &=& \frac{1}{2} \pa{ X_k + X_k ^{-1} A}
        \end{matrix} \right.
    \]
  \end{block}

  \begin{myteo}
    If the spectrum  of $A^{-1/2} X_0$ lies in the right half-plane
    then $X_k$ converges quadratically to $A^{1/2}$.
  \end{myteo}
  \begin{myoss}
    In exact arithmetic every $X_k$ commutes with $A$, in machine
    arithmetic this propriety may be lost.
  \end{myoss}
\end{frame}


% \begin{frame}{Newton's method convergence}
%   \begin{myteo}
%     Let $A \in \mathbb{C}^{n\times n}$ have no eigenvaules on
%     $\mathbb{R}^-$. The Newton iterates $X_k$ with any $X_0$ that
%     commutes with $A$ are related to the Newton sign iterates:
%     \begin{align*}
%       S_{k+1} = & \frac{1}{2} \pa{ S_k + S_k^{-1}} \\
%       S_0 = & A^{-1/2} X_0 
%     \end{align*}
%     by $X_k = A^{1/2} S_k$.
%   \end{myteo}
%   \begin{proof}
%     By induction on $k$:
%     \[ X_{k+1} = \frac{1}{2}\pa{A^{1/2}S_k + S_k^{-1}A^{-1/2}A} =
%         A^{1/2}\frac{1}{2}\pa{S_k + S_k ^{-1}} \]
%   \end{proof}
% \end{frame}

% \begin{frame}{Newton's method convergence}
%   \begin{myteo}
%     If $A^{-1/2}X_0$ has no pure imaginary eigenvaules, the $X_k$
%     are defined and $X_k$ converges quadratically to
%     $A^{1/2}\sign\pa{A^{-1/2} X_0}$.

%     If the spectrum  of $A^{-1/2} X_0$ lies in the right half-plane
%     then $X_k$ converges quadratically to $A^{1/2}$ and, for any
%     consistent norm,
%     \[ \norm{X_{k+1} - A^{1/2}} \le \frac{1}{2}\norm{X_k ^{-1}}
%       \norm{X_k - A^{1/2}} ^2 \]
%   \end{myteo}
%   \vfill
  
%   The hypothesis that $A$ has no eigenvaules in $\mathbb{R}^-$ and our
%   choice of the principal square root function imply that the spectrum
%   of $A^{1/2}$ is contained in the open right half-plane.
% \end{frame}

% \begin{frame}{Newton's method speed of convergence}
%   We have proved that the Newton's method converges for any choice of
%   $X_0$ that commutes with $A$, moreover it converges to $A^{1/2}$
%   quadratically for $X_0 = A$.
%   \vfill
  
%   It's known that the Newton iteration for $\sign\pa{M}$ requires many
%   iterations when $M$ has an eigenvalue close to the imaginary axis.

%   Recalling the relation $X_k = A^{1/2} S_k$ we can conclude that our
%   Newton iteration requires many iterations when $A$ has an eigenvalue
%   close to the negative real axis.  
% \end{frame}

\subsection{DB iteration}

\begin{frame}{Denman and Beavers (DB) iteration}
  If we set $Y_k = A^{-1} X_k$ we can write
  \[ X_{k+1} = \frac{1}{2}\pa{X_k + Y_k ^{-1}} \]
  \[ Y_{k+1} = A^{-1} X_{k+1} = \frac{1}{2}\pa{ Y_k + X_k^{-1}} \]
  (using, again, the fact that $X_k$ commutes with $A$)

  \begin{block}{DB iteration}
    \begin{align*}
      X_{k+1} &= \frac{1}{2} \pa{ X_k + Y_k ^{-1}} & X_0 &= A \\
      Y_{k+1} &= \frac{1}{2} \pa{ Y_k + X_k ^{-1}} & Y_0 &= I 
    \end{align*}
  \end{block}
\end{frame}

\begin{frame}{DB convergence}
  The DB iteration is an algebraic manipulation of the Newton
  iteration, so if $A$ has no eigenvalues on $\mathbb{R}^-$ then we
  can write:
  \begin{align*}
    \lim _{k \to \infty} X_k &= A^{1/2} \\
    \lim _{k\to \infty} Y_k &= A^{-1/2}
  \end{align*}
  \vfill

  In exact arithmetic it has the same proprieties of the Newton
  iterations.
\end{frame}

\begin{frame}{Product form of DB iteration}
  If we set $M_k$ = $X_kY_k$ then we have
  \[ M_{k+1} = \frac{1}{4}\pa{2 I + X_kY_k + Y_k^{-1}X_k^{-1}} =
    \frac{1}{4}\pa{2I + M_k + M_k^{-1}} \]
  \begin{block}{Product form of DB iteration}
    \begin{align*}
      M_{k+1} &= \frac{1}{2}\pa{ I + \frac{M_k + M_k^{-1}}{2}} & M_0 &=
                                                                      A
      \\
      X_{k+1} &= \frac{1}{2} X_k\pa{I+M_k^{-1}} & X_0 &= A \\
      Y_{k+1} &= \frac{1}{2} Y_k\pa{I+M_k^{-1}} & Y_0 &= I
    \end{align*}
  \end{block}
  The $X_k$ and $Y_k$ limits still hold and we have $\lim _{k\to
    \infty} M_k = I$.
  \note{Facciamo un'inversione di matrice in meno ma un prodotto in pi√π}
\end{frame}

\subsection{CR method}

\begin{frame}{CR and IN methods derivation}
  From the Newton method we can write:
  \[ E_{k} = X_{k+1} - X_k = \frac{1}{2}\pa{X_k + X_k^{-1}A} - X_k =
    \frac{1}{2} X_k^{-1} \pa{A-X_k^2} \]
  
  From this we obtain:
  \begin{align*}
    E_{k+1} &= \frac{1}{2}X_{k+1}^{-1}\pa{A-X_{k+1}^2} \\
    &= \frac{1}{2}X_{k+1}^{-1}\pa{A-\frac{1}{4}\pa{X_k +X_k^{-1}A}^2}
    \\
    &= \frac{1}{2}X_{k+1}^{-1}\pa{\frac{2A-X_k^2 -X_k^{-2}A^2}{4}} \\
    &= -\frac{1}{2}X_{k+1}^{-1}\pa{\frac{X_k - X_k^{-1}A}{2}}^2  \\
    &= -\frac{1}{2}X_{k+1}^{-1}E_k^2 = -\frac{1}{2}E_kX_{k+1}^{-1}E_k
  \end{align*}
\end{frame}

\begin{frame}{Incremental iteration}
  \begin{block}{IN iteration}
    \begin{align*}
      X_{k+1} &= X_k + E_k & X_0 &= A \\
      E_{k+1} &= -\frac{1}{2} E_k X_{k+1}^{-1} E_k & E_0
                                 &=\frac{1}{2}\pa{I-A}
    \end{align*}
  \end{block}
  With 
  \begin{align*}
    \lim _{k\to \infty} X_k & = A^{1/2} & \lim _{k\to \infty} E_k &=0
  \end{align*}
\end{frame}

\begin{frame}{CR iteration}
  Setting $Y_k = 2E_k$ and $Z_k = 4X_{k+1}$ in the previous relation
  we obtain
  \begin{block}{CR iteration}
    \begin{align*}
      Y_{k+1} &= -Y_kZ_k^{-1}Y_k & Y_0 &= I -A \\
      Z_{k+1} &= Z_k + 2Y_{k+1} & Z_0 &= 2(I+A)
    \end{align*}
  \end{block}
  With 
  \begin{align*}
    \lim _{k\to \infty} Y_k & = 0 & \lim _{k\to \infty} Z_k &=
                                                              4A^{1/2}
  \end{align*}
\end{frame}

\subsection{Methods comparison}

\begin{frame}{Methods comparison}
  \begin{figure}
    \begin{tabular}{ r | c c }
      Iteration & Operations & Flops \\
      \hline
      Newton & D & $\frac{8}{3}n^3$ \\
      DB & 2I & $4n^3$ \\
      Product DB & M+I & $4n^3$ \\
      CR & M+D & $\frac{14}{3}n^3$ \\
      IN & M+D & $\frac{14}{3}n^3$
    \end{tabular}
  \end{figure}
  \begin{itemize}
  \item \textbf{M}: matrix multiplication;
  \item \textbf{I}: matrix inversion;
  \item \textbf{D}: solution of a multiple right-hand side linear system.
  \end{itemize}
\end{frame}

\begin{frame}{Singular matrices}
  If $A$ is singular (but with semisimple zero eigenvalues) the square
  root still exists.

  Our iterative methods may fail on the first step (they may require
  the inversion of a singual matrix), it can be proved that if we
  start from the second step then the methods converge (in exact
  arithmetic).  \vfill
  
  While convergence holds in exact arithmetic, numerical instability
  is likely. There are ways to modify the methods in order to avoid
  this.
\end{frame}

\section{Stability and accuracy}

\begin{frame}{Pure Newton method stability}
  \begin{align*}
    X_k E_k + E_k X_k &= A - X_k ^2 \\
    X_{k+1} &= X_k + E_k
  \end{align*}
  From standard converge theory we know that, under our hypothesis,
  ``pure'' Newton's method is stable because (sufficiently small)
  rouding errors of one step are damped out in the next. \vfill

  We proved that our version of the Newton's method works when $X_k$
  commutes with $A$ and it is true for every $k$ if $X_0$ satisfies
  that condition. Rounding error may take $X_k$ outside that path and
  the error may be amplified at every step.
\end{frame}

\begin{frame}{Newton iteration stability}
  The iteration function is $g(X) = \frac{1}{2}\pa{X+ X^{-1}A}$, its
  Fr\'echet derivative in $X=A^{1/2}$ is
  \[ L_g\pa{A^{1/2},E} = \frac{1}{2}\pa{E-A^{-1/2}EA^{1/2}} \]
  the eigenvalues of the Kronecker matrix $\tilde L$ are:
  \[ \frac{1}{2}\pa{1-\lambda _i ^{1/2} \lambda _j^{-1/2}} \]
  where $\lambda _i$ are the eigenvales of $A$. So we need
  \[ \max _{i,j} \frac{1}{2} \abs{ 1- \lambda _i^{1/2} \lambda
      _j^{-1/2}} <1 \]
\end{frame}

\begin{frame}{Newton iteration instability}
  It can be shown that the Newton iteration can diverge when started
  arbitrarily close to $A^{1/2}$ when the previous condition is not
  satisfied. \vfill

  The above restriction is servere, for example it can be shown that
  if $A$ is Hermitian positive definite then the condition is
  equivalent to $\kappa (A) <9$.
\end{frame}

\begin{frame}{Newton iteration accuracy}
  Taking $\norm{E} \le \norm{A^{1/2}} u$ we can write:
  \begin{align*}
    \norm{L_g\pa{A^{1/2},E}} &= \frac{1}{2}\norm{E-A^{-1/2}EA^{1/2}} \\
      &\le \frac{1}{2}\pa{\norm{E} +
        \norm{A^{-1/2}}\norm{E}\norm{A^{1/2}}} \\
    & \le \frac{1}{2}\norm{A^{1/2}}\pa{1+\kappa\pa{A^{1/2}}}u
  \end{align*}
  getting the following estimate
  \begin{block}{Newton iteration relative limiting accuracy}
    \[ \frac{1}{2}\pa{1+\kappa\pa{A^{1/2}}}u \]
  \end{block}
\end{frame}

\begin{frame}{DB iteration stability}
  The iteration function is
  \[ G\pa{\bra{\begin{matrix}X\\Y\end{matrix}}} = \frac{1}{2}\bra{
      \begin{matrix}
        X+Y^{-1} \\
        Y+X^{-1}
      \end{matrix}
    } \]
  Any point in the form $\pa{B,B^{-1}}$ is a fixed point, and such
  point the derivative is
  \[ L_g\pa{ \bra{ \begin{matrix}B\\B^{-1}\end{matrix}},
      \bra{\begin{matrix}E\\F\end{matrix}}} = \frac{1}{2}
    \bra{\begin{matrix} E-BFB \\ F-B^{-1}EB^{-1}\end{matrix}} \]

  It's easy to see that $L\pa{ \pa{B,B^{-1}}, \cdot}$ is idempotent,
  in particular it's idempotent at the fixed point $\pa{A^{1/2},A^{-1/2}}$
  and so the method is stable.
\end{frame}

\begin{frame}{DB iteration accuracy}
  Setting $\norm{E} \le u\norm{A^{1/2}}$ and $\norm{F} \le
  u\norm{A^{-1/2}}$ we have
  \[ \norm{\frac{1}{2}\pa{E-A^{1/2}FA^{1/2}}} \le
    \frac{1}{2}\norm{A^{1/2}}\pa{1+ \kappa\pa{A^{1/2}}}u \]
  And a similar relation for the second block of $L_g$
  \vfill

  So the relative limiting accuracy estimate is
  \begin{block}{DB relative accuracy}
    \[ \frac{1}{2}\pa{ 1 + \kappa\pa{A^{1/2}}}u \]
  \end{block}
\end{frame}

\begin{frame}{Product DB iteration stability and accuracy}
  \[ G\pa{\bra{\begin{matrix}M\\X\end{matrix}}} = \frac{1}{2} \bra{
      \begin{matrix}
        I+ \frac{1}{2}\pa{M+M^{-1}} \\
        X\pa{I+M^{-1}}
      \end{matrix}} \]
  \[ L_g\pa{ \bra{ \begin{matrix}I\\X\end{matrix}},
      \bra{\begin{matrix}E\\F\end{matrix}}} = \bra{\begin{matrix}0\\
        F-\frac{1}{2}XE\end{matrix}} \]
  Again it is idempotent and so the iteration is stable at the fixed
  point $\pa{I,A^{1/2}}$. \vfill

  From this formula we could compute a limiting accuracy estimate
  indipendent of $A$:
  \begin{block}{Product DB relative accuracy}
    \[ \frac{3}{2}u \]
  \end{block}
\end{frame}

\begin{frame}{IN iteration stability and accuracy}
  \[ G\pa{\bra{\begin{matrix}X\\E\end{matrix}}} = \bra{
      \begin{matrix}
        X+E  \\
        -\frac{1}{2} E\pa{X+E}^{-1}E
      \end{matrix}} \]
  \[ L_g\pa{ \bra{ \begin{matrix}A^{1/2}\\0\end{matrix}},
      \bra{\begin{matrix}E\\F\end{matrix}}} = \bra{\begin{matrix}E+F\\
        0 \end{matrix}} \]
  It's idempotent and so it's stable. \vfill
  
  
  \begin{block}{CR method relative accuracy}
    \[ u \]
  \end{block}
\end{frame}

\begin{frame}{CR iteration stability and accuracy}
  \[ G\pa{\bra{\begin{matrix}Y\\Z\end{matrix}}} = \bra{
      \begin{matrix}
        -YZ^{-1}Y  \\
        Z-2YZ^{-1}Y
      \end{matrix}} \]
  \[ L_g\pa{ \bra{ \begin{matrix}0\\Z\end{matrix}},
      \bra{\begin{matrix}E\\F\end{matrix}}} = \bra{\begin{matrix}0\\
        F \end{matrix}} \]
  It's idempotent and so it's stable. \vfill
  
  Recalling that $Z = 4A^{1/2}$ we get:
  \begin{block}{CR method accuracy}
    \[ u \]
  \end{block}
\end{frame}

\begin{frame}{Methods comparison}
  \begin{figure}
    \begin{tabular}{ r | c c >{\small}c }
      Iteration & Stability & Accuracy & Flops \\
      \hline
      Newton & Under conditions & $\frac{1}{2}\pa{1+ \kappa\pa{A^{1/2}}}u$ & $\frac{8}{3}n^3$ \\
      DB & Yes & $\frac{1}{2}\pa{1+ \kappa\pa{A^{1/2}}}u$ & $4n^3$ \\
      Prod DB & Yes & $\frac{3}{2}u$ & $4n^3$ \\
      IN & Yes & $u$ & $\frac{14}{3}n^3$ \\
      CR & Yes & $u$ & $\frac{14}{3}n^3$
    \end{tabular}
  \end{figure}
\end{frame}

\section{Numerical experiments}

\subsection{Slow convergence}

\begin{frame}{Slow matrices}
  We've seen that the iterations based on the Newton iteration are
  slow if the input matrix has eigenvalues near $\mathbb{R}^-$.
  \vfill

  So we analize the number of iterations required for the matrix
  
  \[ A = V \bra{ \begin{matrix}
        1 & & \\ 
        & 2 & \\ 
        & & \begin{matrix}
          -1 & \varepsilon \\
          - \varepsilon & -1
        \end{matrix}
      \end{matrix}
    } V^{T}
  \]
  where $V$ is a random orthogonal matrix.

  The matrix $A$ has eigenvalues:
  \[ \set{1,2,-1+\varepsilon i,-1 -\varepsilon} \]
\end{frame}

\begin{frame}
  \includegraphics[width=\textwidth,height=\textheight]{"eig_orto"}
\end{frame}


\subsection{Testing methods}

\begin{frame}{Testing methods}
  For each matrix $A$ to be tested we compute the square root with the
  Schur method, the product DB method and the CR method (the last two
  with a $10^{-30}$ tolerance). We call $S$ the ``best'' approximation
  that minimizes the following quantity:
  \[ \norm{S^2 - A}_F \]
  \vfill

  Then for each method with iterate on the matrix $A$ with a tolerance
  of $10^{-16}$ (for at most $100$ iterations) and we compute the
  absolute error, relative error and residue for the final iteration
  and the best iteration (best for the absolute error).
\end{frame}

\begin{frame}{Plot}
  For each method we plot (with a logarithmic $y$ scale) the following
  quantities for each iteration:
  \begin{itemize}
  \item Residues the stopping criterion used by the method:
    \begin{itemize}
    \item Newton and DB: $\norm{X_k ^2 - A}$,
    \item Product DB: $\norm{M_k -I}$,
    \item CR: $\norm{Y}$;
    \end{itemize}
  \item Commutativity:
    \begin{itemize}
    \item Newton: $\norm{AX_k - X_kA}$,
    \item DB: $\norm{X_kY_k - Y_kX_k}$,
    \item Product DB: $\norm{M_kX_k - X_k,M_k}$,
    \item CR: $\norm{Y_kZ_k - Z_kY_k}$;
    \end{itemize}
  \item Absolute error: $\norm{X_k - S}$.
  \end{itemize}
\end{frame}


\subsection{First example}

\begin{frame}{First example}
  Let $A = I + uv^T$ where $u=\bra{ 1\ 2^2\ \dots\ n^2}$ and $v =
  \bra{ 0\ 1\ 2^2\ \dots\ (n-1)^2}$. We have $\psi_n(A) = 4.92e+04$ and
  $\kappa (A^{1/2}) = 2.22e+02$.
  
  \begin{tabular}{r| c c c c}
    Method & Index & Absolute error & Relative error & Square residue \\
    \hline
    Newton & $100$ & $inf$ & $inf$ & $inf$ \\
    Newton & $10$ & $4.37e-02$ & $1.97e-04$ & $1.93e+01$ \\
    \hline
    DB & $100$ & $2.11e-11$ & $9.49e-14$ & $9.26e-09$ \\
    DB & $13$ & $2.60e-12$ & $1.17e-14$ & $1.18e-10$ \\
    \hline
    Prod DB & $13$ & $4.08e-12$ & $1.84e-14$ & $4.69e-10$ \\
    Prod DB & $12$ & $4.08e-12$ & $1.84e-14$ & $4.69e-10$ \\
    \hline
    CR & $13$ & $1.44e-11$ & $6.49e-14$ & $6.22e-09$ \\
    CR & $12$ & $1.44e-11$ & $6.49e-14$ & $6.22e-09$ \\
  \end{tabular}
\end{frame}

\begin{frame}{Eigenvalues}
  It has one $5e4$ eigenvaule and the other are $1$.
  \includegraphics[width=\textwidth,height=\textheight]{"e1/eigs"}
\end{frame}

\begin{frame}{Newton}
  \includegraphics[width=\textwidth,height=\textheight]{"e1/Newton - absplot"}
\end{frame}
\begin{frame}{DB}
  \includegraphics[width=\textwidth,height=\textheight]{"e1/DB - absplot"}
\end{frame}
\begin{frame}{Product DB}
  \includegraphics[width=\textwidth,height=\textheight]{"e1/Product DB - absplot"}
\end{frame}
\begin{frame}{CR}
  \includegraphics[width=\textwidth,height=\textheight]{"e1/CR - absplot"}
\end{frame}

\subsection{Moler}

\begin{frame}{Moler}
  Let $A = \mathrm{gallery}('moler',16)$, it is symmetric positive
  definite with $\psi_n(A) = 8.64e+01$ and $\kappa(A^{1/2}) = 2.04e+05$.

  \begin{tabular}{r| c c c c}
    Method & Index & Absolute error & Relative error & Square residue \\
    \hline
    Newton & $100$ & $2.32e+10$ & $1.99e+09$ & $8.15e+03$ \\
    Newton & $9$ & $2.47e-03$ & $2.12e-04$ & $1.47e-02$ \\
    \hline
    DB & $100$ & $5.31e-09$ & $4.55e-10$ & $3.05e-08$ \\
    DB & $62$ & $5.31e-09$ & $4.55e-10$ & $3.04e-08$ \\
    \hline
    Prod DB & $19$ & $1.45e-09$ & $1.25e-10$ & $5.71e-09$ \\
    Prod DB & $18$ & $1.45e-09$ & $1.25e-10$ & $5.71e-09$ \\
    \hline
    CR & $19$ & $0.00e+00$ & $0.00e+00$ & $7.74e-14$ \\
    CR & $18$ & $0.00e+00$ & $0.00e+00$ & $7.74e-14$ \\
  \end{tabular}  
\end{frame}

\begin{frame}{Eigenvalues}
  It has $15$ eigenvalues of order $1$ and a small eigenvalue of order $10^{-9}$.
  \includegraphics[width=\textwidth,height=\textheight]{"moler/eigs"}
\end{frame}

\begin{frame}{Newton}
  \includegraphics[width=\textwidth,height=\textheight]{"moler/Newton - absplot"}
\end{frame}
\begin{frame}{DB}
  \includegraphics[width=\textwidth,height=\textheight]{"moler/DB - absplot"}
\end{frame}
\begin{frame}{Product DB}
  \includegraphics[width=\textwidth,height=\textheight]{"moler/Product DB - absplot"}
\end{frame}
\begin{frame}{CR}
  \includegraphics[width=\textwidth,height=\textheight]{"moler/CR - absplot"}
\end{frame}


\subsection{Chebyshev-Vandermonde}

\begin{frame}{Chebyshev-Vandermonde}
  Let $A = \mathrm{gallery}('chebvand',16)$, it has $\psi_n(A) = 4.36$
  and $\kappa(A^{1/2}) = 3.91e+06$

  \begin{tabular}{r| c c c c}
    Method & Index & Absolute error & Relative error & Square residue \\
    \hline
    Newton & $100$ & $9.72e+08$ & $1.55e+08$ & $1.14e+09$ \\
    Newton & $6$ & $5.86e-02$ & $9.35e-03$ & $6.01e-02$ \\
    \hline
    DB & $100$ & $7.05e-05$ & $1.12e-05$ & $1.81e-04$ \\
    DB & $20$ & $7.05e-05$ & $1.12e-05$ & $1.81e-04$ \\
    \hline
    Prod DB & $23$ & $7.16e-05$ & $1.14e-05$ & $1.69e-04$ \\
    Prod DB & $23$ & $7.16e-05$ & $1.14e-05$ & $1.69e-04$ \\
    \hline
    CR & $23$ & $0.00e+00$ & $0.00e+00$ & $5.36e-15$ \\
    CR & $22$ & $0.00e+00$ & $0.00e+00$ & $5.36e-15$ \\
\end{tabular}
\end{frame}

\begin{frame}{Eigenvalues}
  It has $8$ complex eigenvalues with modules of order $1$ and $8$
  real positive eigenvalues between $3.6$ and $10^{-11}$.
  \includegraphics[width=\textwidth,height=\textheight]{"chebvand/eigs"}
\end{frame}

\begin{frame}{Newton}
  \includegraphics[width=\textwidth,height=\textheight]{"chebvand/Newton - absplot"}
\end{frame}
\begin{frame}{DB}
  \includegraphics[width=\textwidth,height=\textheight]{"chebvand/DB - absplot"}
\end{frame}
\begin{frame}{Product DB}
  \includegraphics[width=\textwidth,height=\textheight]{"chebvand/Product DB - absplot"}
\end{frame}
\begin{frame}{CR}
  \includegraphics[width=\textwidth,height=\textheight]{"chebvand/CR - absplot"}
\end{frame}

\end{document}

